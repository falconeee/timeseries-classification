{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc3274c",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, balanced_accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import plotly.graph_objects as go\n",
    "from utils.futurai_ppd import drop_transitorio_desligado\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from collections import Counter\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh import select_features\n",
    "from scipy.stats import kurtosis\n",
    "from sktime.datatypes._panel._convert import from_long_to_nested\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, GlobalMaxPooling1D, LayerNormalization, MultiHeadAttention, Add, Flatten, Concatenate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05d315",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'Depurador 762-28-006 - Cozimento'\n",
    "timestamp = \"Timestamp\"\n",
    "\n",
    "df_dataset = pd.read_csv('data/' + base_name + '.csv', sep=\";\", decimal=\".\", encoding=\"utf-8-sig\")\n",
    "df_dataset[timestamp] = pd.to_datetime(df_dataset[timestamp], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "## Drop columns with NaN values, constant values or irrelevant to the analysis\n",
    "df_dataset.drop(columns=[\"762H0336.PV\", \"762H0342.PV\", \"762N0015.SP\", \"762P0013.SP\", \"762-34-073.CR\", \"762N0015.OP\"], inplace=True, errors='ignore')\n",
    "\n",
    "df_dataset.dropna(inplace=True)\n",
    "\n",
    "print(f\"Dataset shape: {df_dataset.shape}\")\n",
    "\n",
    "list_variables = df_dataset.columns.tolist()\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88f78a",
   "metadata": {},
   "source": [
    "## Remove periods Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5447ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = []\n",
    "pp_var_ref_desligado = \"762-28-006.CR\"\n",
    "pp_valor_ref_desligado = 5\n",
    "pp_tempo_ref_desligado = 0\n",
    "pp_pre_corte_transitorio = 0\n",
    "pp_pos_corte_transitorio = 0\n",
    "pre_process.append(  \n",
    "{\n",
    "   \"after_cut\": pp_pos_corte_transitorio,\n",
    "   \"interval_off\": pp_tempo_ref_desligado,\n",
    "   \"limit_off\": pp_valor_ref_desligado,\n",
    "   \"pre_cut\": pp_pre_corte_transitorio,\n",
    "   \"variable_off\": pp_var_ref_desligado\n",
    "  })\n",
    "\n",
    "for pro in pre_process:\n",
    "    df_dataset,_,_ = drop_transitorio_desligado(df_dataset,pro[\"variable_off\"],pro[\"limit_off\"],pro[\"interval_off\"],timestamp,pre_corte=pro[\"pre_cut\"],pos_corte=pro[\"after_cut\"])\n",
    "print(f\"Dataset shape: {df_dataset.shape}\")\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e0243",
   "metadata": {},
   "source": [
    "## Create label for anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c599f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodos_de_Falhas = [\n",
    "    (pd.Timestamp('2024-05-03 11:00:00'), pd.Timestamp('2024-05-03 11:35:00')),\n",
    "    (pd.Timestamp('2024-06-25 17:20:00'), pd.Timestamp('2024-08-02 14:00:00')),\n",
    "    (pd.Timestamp('2024-10-19 10:40:00'), pd.Timestamp('2024-10-19 10:50:00')),\n",
    "    (pd.Timestamp('2024-10-19 10:40:00'), pd.Timestamp('2024-10-19 10:50:00')),\n",
    "    (pd.Timestamp('2024-10-21 12:00:00'), pd.Timestamp('2024-10-22 00:35:00')),\n",
    "    (pd.Timestamp('2024-10-24 03:10:00'), pd.Timestamp('2024-10-27 00:00:00')),\n",
    "    (pd.Timestamp('2024-11-14 06:40:00'), pd.Timestamp('2024-11-14 19:45:00')),\n",
    "    (pd.Timestamp('2024-11-25 21:45:00'), pd.Timestamp('2024-11-25 22:03:00')),\n",
    "    (pd.Timestamp('2024-11-27 15:00:00'), pd.Timestamp('2024-11-27 15:07:00')),\n",
    "    (pd.Timestamp('2024-11-27 16:04:00'), pd.Timestamp('2024-11-26 16:11:00')),\n",
    "    (pd.Timestamp('2024-11-30 13:30:00'), pd.Timestamp('2024-11-30 15:52:00')),\n",
    "    (pd.Timestamp('2024-12-09 20:30:00'), pd.Timestamp('2024-12-11 07:00:00')),\n",
    "    (pd.Timestamp('2025-03-05 15:17:00'), pd.Timestamp('2025-03-05 15:24:00')),\n",
    "    (pd.Timestamp('2025-03-15 18:30:00'), pd.Timestamp('2025-03-15 19:15:00')),\n",
    "    (pd.Timestamp('2025-03-18 11:40:00'), pd.Timestamp('2025-03-18 20:00:00')),\n",
    "    (pd.Timestamp('2025-06-16 15:20:00'), pd.Timestamp('2025-06-17 07:38:00')),\n",
    "]\n",
    "\n",
    "df_dataset['Falhas'] = 0\n",
    "for inicio, fim in periodos_de_Falhas:\n",
    "    df_dataset.loc[(df_dataset[timestamp] >= inicio) & (df_dataset[timestamp] <= fim), 'Falhas'] = 1\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d334e30",
   "metadata": {},
   "source": [
    "## Import TAGs and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subsistema = pd.read_csv('data/'+ base_name + '_subsistema.csv', sep=\";\", decimal=\".\", encoding=\"utf-8-sig\")\n",
    "df_subsistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b24605",
   "metadata": {},
   "source": [
    "## Plot variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_dataset['Timestamp'],\n",
    "    y=df_dataset[\"762P0034.PV\"],\n",
    "    mode='lines',\n",
    "    name='762P0034.PV',\n",
    "    line=dict(color='black')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_dataset['Timestamp'],\n",
    "    y=df_dataset[\"Falhas\"],\n",
    "    mode='lines',\n",
    "    name='Falhas',\n",
    "    line=dict(color='red')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf4fc",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14afcb",
   "metadata": {},
   "source": [
    "### Extract Features with tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dataset.set_index(timestamp)\n",
    "\n",
    "variables = df.drop(columns=[\"Falhas\"])\n",
    "labels = df[\"Falhas\"]\n",
    "\n",
    "# parâmetros de janela\n",
    "window = pd.Timedelta(\"60min\")\n",
    "step = pd.Timedelta(\"30min\")\n",
    "\n",
    "# criar índices baseados em passo fixo\n",
    "df = df.copy()\n",
    "df[\"window_start\"] = df.index.floor(step)\n",
    "df[\"window_id\"] = ((df.index - df[\"window_start\"]) // step).cumsum()\n",
    "\n",
    "# corrigir para garantir tamanho de janela correto\n",
    "valid_windows = df.groupby(\"window_id\").filter(\n",
    "    lambda x: (x.index.max() - x.index.min()) >= window\n",
    ")\n",
    "\n",
    "# long format direto\n",
    "long_format = (\n",
    "    valid_windows\n",
    "    .reset_index()\n",
    "    .melt(id_vars=[timestamp, \"window_id\"], var_name=\"variable\", value_name=\"value\")\n",
    ")\n",
    "\n",
    "# extrair features\n",
    "X_features = extract_features(\n",
    "    long_format,\n",
    "    column_id=\"window_id\",\n",
    "    column_sort=timestamp,\n",
    "    column_kind=\"variable\",\n",
    "    column_value=\"value\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "X_features = impute(X_features)\n",
    "\n",
    "# rótulo: 1 se houver falha dentro da janela\n",
    "y_features = labels.groupby(df[\"window_id\"]).apply(lambda x: int(x.any()))\n",
    "y_features = y_features.loc[X_features.index]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)\n",
    "print(\"Shape final de y_features:\", y_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ccf96",
   "metadata": {},
   "source": [
    "### Extract Features with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf16d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "def shannon_entropy(arr, bins=10):\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    p, _ = np.histogram(arr, bins=bins, density=True)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return np.nan\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def safe_autocorr(s, lag=1):\n",
    "    try:\n",
    "        return s.autocorr(lag=lag)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# function to extract features from a window\n",
    "def features_from_window(g):\n",
    "    out = {}\n",
    "    for col in g.columns:\n",
    "        s = g[col].dropna()\n",
    "        prefix = col\n",
    "        if s.empty:\n",
    "            for name in [\"mean\",\"std\",\"var\",\"min\",\"max\",\"median\",\"skew\",\"kurtosis\",\n",
    "                         \"q25\",\"q75\",\"iqr\",\"amplitude\",\"energy\",\"entropy\",\n",
    "                         \"autocorr_lag1\",\"autocorr_lag2\",\"coeff_var\"]:\n",
    "                out[f\"{prefix}_{name}\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        arr = s.values\n",
    "        out[f\"{prefix}_mean\"] = s.mean()\n",
    "        out[f\"{prefix}_std\"] = s.std()\n",
    "        out[f\"{prefix}_var\"] = s.var()\n",
    "        out[f\"{prefix}_min\"] = s.min()\n",
    "        out[f\"{prefix}_max\"] = s.max()\n",
    "        out[f\"{prefix}_median\"] = s.median()\n",
    "        out[f\"{prefix}_skew\"] = s.skew()\n",
    "        out[f\"{prefix}_kurtosis\"] = float(kurtosis(arr, fisher=True))\n",
    "        out[f\"{prefix}_q25\"] = np.percentile(arr, 25)\n",
    "        out[f\"{prefix}_q75\"] = np.percentile(arr, 75)\n",
    "        out[f\"{prefix}_iqr\"] = out[f\"{prefix}_q75\"] - out[f\"{prefix}_q25\"]\n",
    "        out[f\"{prefix}_amplitude\"] = out[f\"{prefix}_max\"] - out[f\"{prefix}_min\"]\n",
    "        out[f\"{prefix}_energy\"] = np.sum(arr ** 2)\n",
    "        out[f\"{prefix}_entropy\"] = shannon_entropy(arr, bins=10)\n",
    "        out[f\"{prefix}_autocorr_lag1\"] = safe_autocorr(s, lag=1)\n",
    "        out[f\"{prefix}_autocorr_lag2\"] = safe_autocorr(s, lag=2)\n",
    "        mean_val = out[f\"{prefix}_mean\"]\n",
    "        out[f\"{prefix}_coeff_var\"] = (out[f\"{prefix}_std\"] / mean_val) if mean_val not in (0, np.nan) else np.nan\n",
    "\n",
    "    return pd.Series(out)\n",
    "\n",
    "\n",
    "# params\n",
    "window = \"60min\"\n",
    "step = \"30min\"\n",
    "\n",
    "df = df_dataset.set_index(timestamp)\n",
    "variables = df.drop(columns=[\"Falhas\"])\n",
    "labels = df[\"Falhas\"]\n",
    "\n",
    "# build sliding windows\n",
    "starts = pd.date_range(df.index.min(), df.index.max() - pd.Timedelta(window), freq=step)\n",
    "\n",
    "X_list, y_list, idx_list = [], [], []\n",
    "\n",
    "for start in starts:\n",
    "    end = start + pd.Timedelta(window)\n",
    "    window_vars = variables.loc[start:end]\n",
    "    window_labels = labels.loc[start:end]\n",
    "\n",
    "    if window_vars.empty or window_labels.empty:\n",
    "        continue\n",
    "\n",
    "    # extract features\n",
    "    feats = features_from_window(window_vars)\n",
    "    X_list.append(feats)\n",
    "\n",
    "    # if any failure in the window, label as 1\n",
    "    y_list.append(int(window_labels.any()))\n",
    "\n",
    "    # index representing the center of the window\n",
    "    idx_list.append(start + (pd.Timedelta(window) / 2))\n",
    "\n",
    "# build final DataFrames\n",
    "X_features = pd.DataFrame(X_list, index=idx_list)\n",
    "y_features = pd.Series(y_list, index=idx_list, name=\"Falhas\")\n",
    "\n",
    "# remove samples with any NaN in X_features\n",
    "mask_valid = ~X_features.isna().any(axis=1)\n",
    "X_features = X_features.loc[mask_valid]\n",
    "y_features = y_features.loc[mask_valid]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)\n",
    "print(\"Shape final de y_features:\", y_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44488027",
   "metadata": {},
   "source": [
    "### Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a401316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dataset.set_index(timestamp)\n",
    "\n",
    "variables = df.drop(columns=[\"Falhas\"])\n",
    "labels = df[\"Falhas\"]\n",
    "\n",
    "# windows parameters\n",
    "window = pd.Timedelta(\"60min\")\n",
    "step = pd.Timedelta(\"30min\")\n",
    "\n",
    "# init point of each window\n",
    "starts = pd.date_range(df.index.min(), df.index.max() - window, freq=step)\n",
    "\n",
    "X_list, y_list, idx_list = [], [], []\n",
    "\n",
    "for start in starts:\n",
    "    end = start + window\n",
    "    window_vars = variables.loc[start:end]\n",
    "    window_labels = labels.loc[start:end]\n",
    "\n",
    "    if window_vars.empty or window_labels.empty:\n",
    "        continue\n",
    "\n",
    "    # Extract basic features\n",
    "    feats = window_vars.agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\", \"var\"]).T\n",
    "    feats.index = [f\"{col}\" for col in feats.index]  # keep variable names\n",
    "    feats = feats.stack()  # \"long\"\n",
    "    feats.index = [f\"{var}_{stat}\" for var, stat in feats.index]\n",
    "    X_list.append(feats)\n",
    "\n",
    "    # any failure in the window = label 1\n",
    "    y_list.append(int(window_labels.any()))\n",
    "\n",
    "    # index represent the center of the window\n",
    "    idx_list.append(start + window / 2)\n",
    "\n",
    "# Final DataFrames\n",
    "X_features = pd.DataFrame(X_list, index=idx_list)\n",
    "y_features = pd.Series(y_list, index=idx_list, name=\"Falhas\")\n",
    "\n",
    "# Clean NaN values\n",
    "X_features = X_features.dropna()\n",
    "y_features = y_features.loc[X_features.index]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)\n",
    "print(\"Shape final de y_features:\", y_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64524a3a",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755cc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features_train, X_features_test, y_features_train, y_features_test = train_test_split(\n",
    "    X_features, \n",
    "    y_features, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "## Scalling training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_features_train)\n",
    "X_features_train = pd.DataFrame(scaler.transform(X_features_train), columns=X_features_train.columns, index=X_features_train.index)\n",
    "X_features_test = pd.DataFrame(scaler.transform(X_features_test), columns=X_features_test.columns, index=X_features_test.index)\n",
    "\n",
    "\n",
    "## Verify class distribution in train and test sets\n",
    "print(\"Class distribution in training set (%):\")\n",
    "print(y_features_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in test set (%):\")\n",
    "print(y_features_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01cc46",
   "metadata": {},
   "source": [
    "### SMOTE training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac228a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_features_train_balanced, y_features_train_balanced = sm.fit_resample(X_features_train, y_features_train)\n",
    "\n",
    "## Verify class distribution in train and test sets\n",
    "print(\"Class distribution in training set (%):\")\n",
    "print(y_features_train_balanced.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbbe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_features_train_balanced = X_features_train\n",
    "# y_features_train_balanced = y_features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0293f3",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242eef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400, \n",
    "    criterion='gini', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=None, \n",
    "    random_state=42, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    class_weight=None, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None, \n",
    "    monotonic_cst=None\n",
    ")\n",
    "\n",
    "## Cross validation\n",
    "k_folds = KFold(n_splits = 5)\n",
    "scores = cross_val_score(rf_clf, X_features_train_balanced, y_features_train_balanced, cv = k_folds)\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))\n",
    "\n",
    "## Fit model\n",
    "rf_clf.fit(X_features_train_balanced, y_features_train_balanced)\n",
    "\n",
    "## Predict and evaluate\n",
    "y_pred = rf_clf.predict(X_features_test)\n",
    "\n",
    "accuracy = balanced_accuracy_score(y_features_test, y_pred)\n",
    "f1 = f1_score(y_features_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_features_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_features_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "## Plot ROC Curve\n",
    "y_proba = rf_clf.predict_proba(X_features_test)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_features_test)\n",
    "\n",
    "y_test_bin = y_test_bin.ravel()\n",
    "y_scores = y_proba[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_scores)\n",
    "roc_auc = roc_auc_score(y_test_bin, y_scores)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - RandomForest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d928d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf_clf.predict(X_features_test.to_numpy())\n",
    "\n",
    "y_features_test.sort_index(inplace=True)\n",
    "X_features_test.sort_index(inplace=True)\n",
    "\n",
    "fig_pred_rf = go.Figure()\n",
    "fig_pred_rf.add_trace(go.Scatter(\n",
    "    x = X_features_test.index,\n",
    "    y=y_pred_rf,\n",
    "    name='Predict',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='black')\n",
    "))\n",
    "fig_pred_rf.add_trace(go.Scatter(\n",
    "    y=y_features_test,\n",
    "    x = X_features_test.index,\n",
    "    name='Real',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='red')\n",
    "))\n",
    "fig_pred_rf.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    title=\"RandomForest Predictions vs Real Values\"\n",
    ")\n",
    "fig_pred_rf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(rf_clf)\n",
    "# Shap values of the test set\n",
    "shap_values = explainer.shap_values(X_features_test.to_numpy())\n",
    "\n",
    "# Summary plot mean feature importance\n",
    "shap.summary_plot(shap_values, X_features_test, plot_type=\"bar\")\n",
    "# 2. Beeswarm plot impact of each variable on each prediction\n",
    "shap.summary_plot(shap_values, X_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dcc8c5",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=400,      # number of trees\n",
    "    learning_rate=0.1,     # learning rate (smaller is more stable, but needs more trees)\n",
    "    max_depth=6,           # maximum tree depth\n",
    "    subsample=0.8,         # fraction of samples used in each tree\n",
    "    colsample_bytree=0.8,  # fraction of features used in each tree\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "## Cross validation\n",
    "k_folds = KFold(n_splits = 5)\n",
    "scores = cross_val_score(xgb_clf, X_features_train_balanced.to_numpy(), y_features_train_balanced.to_numpy(), cv = k_folds)\n",
    "print(\"Cross Validation Scores: \", scores)\n",
    "print(\"Average CV Score: \", scores.mean())\n",
    "print(\"Number of CV Scores used in Average: \", len(scores))\n",
    "\n",
    "## Fit model\n",
    "xgb_clf.fit(X_features_train_balanced.to_numpy(), y_features_train_balanced.to_numpy())\n",
    "\n",
    "## Predict and evaluate\n",
    "y_pred = xgb_clf.predict(X_features_test.to_numpy())\n",
    "\n",
    "accuracy = balanced_accuracy_score(y_features_test, y_pred)\n",
    "f1 = f1_score(y_features_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_features_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_features_test, y_pred, average='weighted')\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "## Plot ROC Curve\n",
    "y_proba = xgb_clf.predict_proba(X_features_test.to_numpy())\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_features_test.to_numpy())\n",
    "\n",
    "y_test_bin = y_test_bin.ravel()\n",
    "y_scores = y_proba[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test_bin, y_scores)\n",
    "roc_auc = roc_auc_score(y_test_bin, y_scores)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - RandomForest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e650fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb_clf.predict(X_features_test.to_numpy())\n",
    "\n",
    "y_features_test.sort_index(inplace=True)\n",
    "\n",
    "fig_pred_xgb = go.Figure()\n",
    "fig_pred_xgb.add_trace(go.Scatter(\n",
    "    x = X_features_test.index,\n",
    "    y=y_pred_xgb,\n",
    "    name='Predict',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='black')\n",
    "))\n",
    "fig_pred_xgb.add_trace(go.Scatter(\n",
    "    y=y_features_test,\n",
    "    x = X_features_test.index,\n",
    "    name='Real',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='red')\n",
    "))\n",
    "fig_pred_xgb.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    title=\"XGBoost Predictions vs Real Values\"\n",
    ")\n",
    "fig_pred_xgb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3d4148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_clf)\n",
    "# Shap values of the test set\n",
    "shap_values = explainer.shap_values(X_features_test.to_numpy())\n",
    "\n",
    "# Summary plot mean feature importance\n",
    "shap.summary_plot(shap_values, X_features_test, plot_type=\"bar\")\n",
    "# 2. Beeswarm plot impact of each variable on each prediction\n",
    "shap.summary_plot(shap_values, X_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca542c3c",
   "metadata": {},
   "source": [
    "# New Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a495f7",
   "metadata": {},
   "source": [
    "## Import and pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80984156",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = 'Depurador 762-28-006'\n",
    "timestamp = \"Timestamp\"\n",
    "\n",
    "df_newdata = pd.read_csv('data/' + base_name + '_teste.csv', sep=\";\", decimal=\".\", encoding=\"utf-8-sig\")\n",
    "df_newdata = df_newdata[list_variables]\n",
    "\n",
    "df_newdata[timestamp] = pd.to_datetime(df_newdata[timestamp], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df_newdata.drop_duplicates(subset=[timestamp], keep='first', inplace=True)\n",
    "df_newdata.sort_values(by=timestamp, inplace=True)\n",
    "df_newdata.dropna(inplace=True)\n",
    "print(f\"Dataset shape: {df_newdata.shape}\")\n",
    "\n",
    "\n",
    "pre_process = []\n",
    "pp_var_ref_desligado = \"762-28-006.CR\"\n",
    "pp_valor_ref_desligado = 5\n",
    "pp_tempo_ref_desligado = 0\n",
    "pp_pre_corte_transitorio = 0\n",
    "pp_pos_corte_transitorio = 0\n",
    "pre_process.append(  \n",
    "{\n",
    "   \"after_cut\": pp_pos_corte_transitorio,\n",
    "   \"interval_off\": pp_tempo_ref_desligado,\n",
    "   \"limit_off\": pp_valor_ref_desligado,\n",
    "   \"pre_cut\": pp_pre_corte_transitorio,\n",
    "   \"variable_off\": pp_var_ref_desligado\n",
    "  })\n",
    "\n",
    "for pro in pre_process:\n",
    "    df_newdata ,_,_ = drop_transitorio_desligado(df_newdata,pro[\"variable_off\"],pro[\"limit_off\"],pro[\"interval_off\"],timestamp,pre_corte=pro[\"pre_cut\"],pos_corte=pro[\"after_cut\"])\n",
    "print(f\"Dataset shape after remove offs: {df_newdata.shape}\")\n",
    "df_newdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_newdata['Timestamp'],\n",
    "    y=df_newdata[\"762P0034.PV\"],\n",
    "    mode='lines',\n",
    "    name='762P0034.PV',\n",
    "    line=dict(color='black')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd220a",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d42e62",
   "metadata": {},
   "source": [
    "### Extract Features with tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d10f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_newdata.set_index(timestamp)\n",
    "\n",
    "variables = df_predict.drop(columns=[\"Falhas\"])\n",
    "labels = df_predict[\"Falhas\"]\n",
    "\n",
    "# parâmetros de janela\n",
    "window = pd.Timedelta(\"60min\")\n",
    "step = pd.Timedelta(\"30min\")\n",
    "\n",
    "# criar índices baseados em passo fixo\n",
    "df_predict = df_predict.copy()\n",
    "df_predict[\"window_start\"] = df_predict.index.floor(step)\n",
    "df_predict[\"window_id\"] = ((df_predict.index - df_predict[\"window_start\"]) // step).cumsum()\n",
    "\n",
    "# corrigir para garantir tamanho de janela correto\n",
    "valid_windows = df_predict.groupby(\"window_id\").filter(\n",
    "    lambda x: (x.index.max() - x.index.min()) >= window\n",
    ")\n",
    "\n",
    "# long format direto\n",
    "long_format = (\n",
    "    valid_windows\n",
    "    .reset_index()\n",
    "    .melt(id_vars=[timestamp, \"window_id\"], var_name=\"variable\", value_name=\"value\")\n",
    ")\n",
    "\n",
    "# extrair features\n",
    "X_features = extract_features(\n",
    "    long_format,\n",
    "    column_id=\"window_id\",\n",
    "    column_sort=timestamp,\n",
    "    column_kind=\"variable\",\n",
    "    column_value=\"value\",\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "X_features = impute(X_features)\n",
    "\n",
    "# rótulo: 1 se houver falha dentro da janela\n",
    "y_features = labels.groupby(df_predict[\"window_id\"]).apply(lambda x: int(x.any()))\n",
    "y_features = y_features.loc[X_features.index]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)\n",
    "print(\"Shape final de y_features:\", y_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d06715",
   "metadata": {},
   "source": [
    "### Extract Features with functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "## utils\n",
    "def shannon_entropy(arr, bins=10):\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    p, _ = np.histogram(arr, bins=bins, density=True)\n",
    "    p = p[p > 0]\n",
    "    if p.size == 0:\n",
    "        return np.nan\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "def safe_autocorr(s, lag=1):\n",
    "    try:\n",
    "        return s.autocorr(lag=lag)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# function to extract features from a window\n",
    "def features_from_window(g):\n",
    "    out = {}\n",
    "    for col in g.columns:\n",
    "        s = g[col].dropna()\n",
    "        prefix = col\n",
    "        if s.empty:\n",
    "            for name in [\"mean\",\"std\",\"var\",\"min\",\"max\",\"median\",\"skew\",\"kurtosis\",\n",
    "                         \"q25\",\"q75\",\"iqr\",\"amplitude\",\"energy\",\"entropy\",\n",
    "                         \"autocorr_lag1\",\"autocorr_lag2\",\"coeff_var\"]:\n",
    "                out[f\"{prefix}_{name}\"] = np.nan\n",
    "            continue\n",
    "\n",
    "        arr = s.values\n",
    "        out[f\"{prefix}_mean\"] = s.mean()\n",
    "        out[f\"{prefix}_std\"] = s.std()\n",
    "        out[f\"{prefix}_var\"] = s.var()\n",
    "        out[f\"{prefix}_min\"] = s.min()\n",
    "        out[f\"{prefix}_max\"] = s.max()\n",
    "        out[f\"{prefix}_median\"] = s.median()\n",
    "        out[f\"{prefix}_skew\"] = s.skew()\n",
    "        out[f\"{prefix}_kurtosis\"] = float(kurtosis(arr, fisher=True))\n",
    "        out[f\"{prefix}_q25\"] = np.percentile(arr, 25)\n",
    "        out[f\"{prefix}_q75\"] = np.percentile(arr, 75)\n",
    "        out[f\"{prefix}_iqr\"] = out[f\"{prefix}_q75\"] - out[f\"{prefix}_q25\"]\n",
    "        out[f\"{prefix}_amplitude\"] = out[f\"{prefix}_max\"] - out[f\"{prefix}_min\"]\n",
    "        out[f\"{prefix}_energy\"] = np.sum(arr ** 2)\n",
    "        out[f\"{prefix}_entropy\"] = shannon_entropy(arr, bins=10)\n",
    "        out[f\"{prefix}_autocorr_lag1\"] = safe_autocorr(s, lag=1)\n",
    "        out[f\"{prefix}_autocorr_lag2\"] = safe_autocorr(s, lag=2)\n",
    "        mean_val = out[f\"{prefix}_mean\"]\n",
    "        out[f\"{prefix}_coeff_var\"] = (out[f\"{prefix}_std\"] / mean_val) if mean_val not in (0, np.nan) else np.nan\n",
    "\n",
    "    return pd.Series(out)\n",
    "\n",
    "\n",
    "# params\n",
    "window = \"60min\"\n",
    "step = \"30min\"\n",
    "\n",
    "df_predict = df_newdata.set_index(timestamp)\n",
    "variables = df_predict.drop(columns=[\"Falhas\"], errors='ignore')\n",
    "\n",
    "# build sliding windows\n",
    "starts = pd.date_range(df_predict.index.min(), df_predict.index.max() - pd.Timedelta(window), freq=step)\n",
    "\n",
    "X_list, y_list, idx_list = [], [], []\n",
    "\n",
    "for start in starts:\n",
    "    end = start + pd.Timedelta(window)\n",
    "    window_vars = variables.loc[start:end]\n",
    "    window_labels = labels.loc[start:end]\n",
    "\n",
    "    if window_vars.empty:\n",
    "        continue\n",
    "\n",
    "    # extract features\n",
    "    feats = features_from_window(window_vars)\n",
    "    X_list.append(feats)\n",
    "\n",
    "    # index representing the center of the window\n",
    "    idx_list.append(start + (pd.Timedelta(window) / 2))\n",
    "\n",
    "# build final DataFrames\n",
    "X_features = pd.DataFrame(X_list, index=idx_list)\n",
    "\n",
    "# remove samples with any NaN in X_features\n",
    "mask_valid = ~X_features.isna().any(axis=1)\n",
    "X_features = X_features.loc[mask_valid]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f915a7",
   "metadata": {},
   "source": [
    "### Basic Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = df_newdata.set_index(timestamp)\n",
    "\n",
    "variables = df_predict.drop(columns=[\"Falhas\"])\n",
    "labels = df_predict[\"Falhas\"]\n",
    "\n",
    "# windows parameters\n",
    "window = pd.Timedelta(\"60min\")\n",
    "step = pd.Timedelta(\"30min\")\n",
    "\n",
    "# init point of each window\n",
    "starts = pd.date_range(df_predict.index.min(), df_predict.index.max() - window, freq=step)\n",
    "\n",
    "X_list, y_list, idx_list = [], [], []\n",
    "\n",
    "for start in starts:\n",
    "    end = start + window\n",
    "    window_vars = variables.loc[start:end]\n",
    "    window_labels = labels.loc[start:end]\n",
    "\n",
    "    if window_vars.empty or window_labels.empty:\n",
    "        continue\n",
    "\n",
    "    # Extract basic features\n",
    "    feats = window_vars.agg([\"mean\", \"std\", \"min\", \"max\", \"median\", \"skew\", \"var\"]).T\n",
    "    feats.index = [f\"{col}\" for col in feats.index]  # keep variable names\n",
    "    feats = feats.stack()  # \"long\"\n",
    "    feats.index = [f\"{var}_{stat}\" for var, stat in feats.index]\n",
    "    X_list.append(feats)\n",
    "\n",
    "    # any failure in the window = label 1\n",
    "    y_list.append(int(window_labels.any()))\n",
    "\n",
    "    # index represent the center of the window\n",
    "    idx_list.append(start + window / 2)\n",
    "\n",
    "# Final DataFrames\n",
    "X_features = pd.DataFrame(X_list, index=idx_list)\n",
    "y_features = pd.Series(y_list, index=idx_list, name=\"Falhas\")\n",
    "\n",
    "# Clean NaN values\n",
    "X_features = X_features.dropna()\n",
    "y_features = y_features.loc[X_features.index]\n",
    "\n",
    "print(\"Shape final de X_features:\", X_features.shape)\n",
    "print(\"Shape final de y_features:\", y_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beee471",
   "metadata": {},
   "source": [
    "## RandomForest Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_rf = rf_clf.predict_proba(X_features)[:, 1]\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=X_features.index,\n",
    "    y=pred_proba_rf*100,\n",
    "    mode='lines',\n",
    "    name='RandomForest',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "fig2.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    title='RandomForest',\n",
    "    yaxis_title='Probabilidade de Falha (%)',\n",
    "    xaxis_title='Tempo'\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec325c",
   "metadata": {},
   "source": [
    "## XGBoost Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fae8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_xgb = xgb_clf.predict_proba(X_features.to_numpy())[:, 1]\n",
    "\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(\n",
    "    x=X_features.index,\n",
    "    y=pred_proba_xgb*100,\n",
    "    mode='lines',\n",
    "    name='XGBoost',\n",
    "    line=dict(color='green')\n",
    "))\n",
    "fig3.update_layout(\n",
    "    template='plotly_white',\n",
    "    hovermode='x unified',\n",
    "    title='XGBoost',\n",
    "    yaxis_title='Probabilidade de Falha (%)',\n",
    "    xaxis_title='Tempo'\n",
    ")\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b7a2af",
   "metadata": {},
   "source": [
    "# TimeSeries Classification sktime Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e417e",
   "metadata": {},
   "source": [
    "## ROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fe559",
   "metadata": {},
   "source": [
    "### Format data to sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36522bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data_for_sktime(df, window_duration, overlap_duration, target_col='Falhas'):\n",
    "    \"\"\"\n",
    "    format dataframe to sktime format for time series classification\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with DatetimeIndex.\n",
    "        window_duration (str): window duration.\n",
    "        overlap_duration (str): overlap duration.\n",
    "        target_col (str): target column name.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.Series]:\n",
    "            - X: DataFrame in pd-multiindex format (instance, timestamp).\n",
    "            - y: series with labels for each instance.\n",
    "    \"\"\"\n",
    "    # convert durations to timedelta\n",
    "    window_td = pd.to_timedelta(window_duration)\n",
    "    overlap_td = pd.to_timedelta(overlap_duration)\n",
    "    step_td = window_td - overlap_td # step between windows\n",
    "\n",
    "    # List to store windows\n",
    "    window_list = []\n",
    "    # List to store labels for each window\n",
    "    y_list = []\n",
    "\n",
    "    start_time = df.index[0]\n",
    "    end_time = df.index[-1]\n",
    "    \n",
    "    window_start = start_time\n",
    "    instance_id = 0\n",
    "\n",
    "    while window_start + window_td <= end_time:\n",
    "        window_end = window_start + window_td\n",
    "        \n",
    "        # select data in the current window\n",
    "        window_df = df.loc[window_start:window_end].iloc[:-1] # iloc[:-1] for exactly window size 60\n",
    "        \n",
    "        # garantee window has the correct size\n",
    "        if window_df.shape[0] == window_td.total_seconds() / 60:\n",
    "            # associate instance_id to the window\n",
    "            window_df['instance_id'] = instance_id\n",
    "            window_list.append(window_df)\n",
    "\n",
    "            # define label for the window: 1 if any failure in the window, else 0\n",
    "            # the .any() function is more robust than .mode() for rare events.\n",
    "            label = 1 if window_df[target_col].any() else 0\n",
    "            y_list.append(pd.Series(label, index=[instance_id]))\n",
    "            \n",
    "            instance_id += 1\n",
    "        \n",
    "        # move to next window\n",
    "        window_start += step_td\n",
    "\n",
    "    if not window_list:\n",
    "        print(\"Nenhuma janela completa foi criada. Verifique as durações e o tamanho do DataFrame.\")\n",
    "        return None, None\n",
    "        \n",
    "    # concatenate all windows into a single DataFrame\n",
    "    X_sktime = pd.concat(window_list)\n",
    "    X_sktime = X_sktime.drop(columns=[target_col])\n",
    "    X_sktime = X_sktime.set_index(['instance_id', X_sktime.index])\n",
    "    X_sktime.index.names = ['instances', 'timepoints']\n",
    "\n",
    "    # concatenate labels into a single Series\n",
    "    y_sktime = pd.concat(y_list)\n",
    "    y_sktime.index.name = 'instances'\n",
    "\n",
    "    return X_sktime, y_sktime\n",
    "\n",
    "## apply function\n",
    "window = \"60min\"\n",
    "overlap = \"30min\"\n",
    "\n",
    "df_sktime = df_dataset.copy().set_index(timestamp)\n",
    "X, y = format_data_for_sktime(df_sktime, window, overlap, target_col='Falhas')\n",
    "\n",
    "## display results\n",
    "if X is not None:\n",
    "    print(f\"format of X (features): {X.shape}\")\n",
    "    print(f\"format of y (target): {y.shape}\\n\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc07fc7",
   "metadata": {},
   "source": [
    "### Split train test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedec8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.split import temporal_train_test_split\n",
    "\n",
    "y_train_sktime, y_test_sktime, X_train_sktime, X_test_sktime = temporal_train_test_split(\n",
    "    y, X, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0d560",
   "metadata": {},
   "source": [
    "### Apply ROCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4901d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket = Rocket(num_kernels=600, random_state=42)\n",
    "rocket.fit(X_train_sktime)\n",
    "\n",
    "X_train_feat = rocket.transform(X_train_sktime)\n",
    "X_test_feat = rocket.transform(X_test_sktime)\n",
    "\n",
    "print(\"Shape features após Rocket:\", X_train_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74a3e7",
   "metadata": {},
   "source": [
    "### SMOTE in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da2527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sktime_balanced, y_train_sktime_balanced = sm.fit_resample(X_train_feat, y_train_sktime)\n",
    "\n",
    "print(\"Distribution after SMOTE:\", Counter(y_train_sktime_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a30fee",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RidgeClassifierCV is recommended by the ROCKET paper\n",
    "rdgcv_clf = RidgeClassifierCV(alphas=np.logspace(-4, 4, 100))\n",
    "rdgcv_clf.fit(X_train_sktime_balanced, y_train_sktime_balanced)\n",
    "\n",
    "## Predict\n",
    "y_pred_rocket = rdgcv_clf.predict(X_test_feat)\n",
    "\n",
    "accuracy = balanced_accuracy_score(y_test_sktime, y_pred_rocket)\n",
    "f1 = f1_score(y_test_sktime, y_pred_rocket, average='weighted')\n",
    "recall = recall_score(y_test_sktime, y_pred_rocket, average='weighted')\n",
    "precision = precision_score(y_test_sktime, y_pred_rocket, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "\n",
    "## Plot ROC Curve\n",
    "y_scores = rdgcv_clf.decision_function(X_test_feat)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test_sktime)\n",
    "\n",
    "if y_test_bin.shape[1] == 1:\n",
    "    y_test_bin = y_test_bin.ravel()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_scores)\n",
    "roc_auc = roc_auc_score(y_test_bin, y_scores)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=1, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a36cd0",
   "metadata": {},
   "source": [
    "### Plot predictions X real labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b159a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_features_test.sort_index(inplace=True)\n",
    "\n",
    "fig_pred = go.Figure()\n",
    "fig_pred.add_trace(go.Scatter(\n",
    "    y=y_pred_rocket,\n",
    "    name='Predict',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='black')\n",
    "))\n",
    "fig_pred.add_trace(go.Scatter(\n",
    "    y=y_test_sktime,\n",
    "    name='Real',\n",
    "    mode=\"lines\",\n",
    "    line=dict(color='red')\n",
    "))\n",
    "fig_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba9a1c",
   "metadata": {},
   "source": [
    "## WEALSEL + MUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.classification.dictionary_based import MUSE\n",
    "\n",
    "# Fit Model\n",
    "muse_clf = MUSE(anova=True, variance=False, bigrams=True, window_inc=2, alphabet_size=4, use_first_order_differences=True, feature_selection='chi2', p_threshold=0.05, support_probabilities=False, n_jobs=1, random_state=42)\n",
    "muse_clf.fit(X_train_sktime, y_train_sktime)\n",
    "\n",
    "# Predict\n",
    "y_pred_muse = muse_clf.predict(X_test_sktime)\n",
    "\n",
    "accuracy = balanced_accuracy_score(y_test_sktime, y_pred_muse)\n",
    "f1 = f1_score(y_test_sktime, y_pred_muse, average='weighted')\n",
    "recall = recall_score(y_test_sktime, y_pred_muse, average='weighted')\n",
    "precision = precision_score(y_test_sktime, y_pred_muse, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfd1f5b",
   "metadata": {},
   "source": [
    "# DeepLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d9c30",
   "metadata": {},
   "source": [
    "## Split Train-Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c8d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset.drop(columns=['Falhas', 'Timestamp'])\n",
    "y = df_dataset['Falhas']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "## Scalling training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "\n",
    "## Verify class distribution in train and test sets\n",
    "print(\"Class distribution in training set (%):\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in test set (%):\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1570d14",
   "metadata": {},
   "source": [
    "## Aplly SMOTE in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(X, y, window_size):\n",
    "    X_windows, y_windows = [], []\n",
    "    # Usamos .values para trabalhar com arrays numpy, que é mais rápido\n",
    "    x_vals = X.values\n",
    "    y_vals = y.values\n",
    "    \n",
    "    for i in range(len(X) - window_size + 1):\n",
    "        X_windows.append(x_vals[i:i + window_size])\n",
    "        # Rótulo da janela é 1 se houver qualquer falha nela\n",
    "        y_windows.append(np.any(y_vals[i:i + window_size]))\n",
    "        \n",
    "    return np.array(X_windows), np.array(y_windows).astype(int)\n",
    "\n",
    "window_size = 60\n",
    "X_train_windows, y_train_windows = create_windows(X_train_scaled, y_train, window_size)\n",
    "X_test_windows, y_test_windows = create_windows(X_test_scaled, y_test, window_size)\n",
    "\n",
    "print(f\"Formato das janelas de treino (X): {X_train_windows.shape}\")\n",
    "print(f\"Formato dos rótulos de treino (y): {y_train_windows.shape}\")\n",
    "print(f\"Distribuição de classes no treino (janelas): {Counter(y_train_windows)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 4. APLICAÇÃO DO SMOTE (Nas janelas de treino)\n",
    "# O SMOTE precisa de dados 2D, então achatamos as janelas temporariamente\n",
    "n_samples, n_timesteps, n_features = X_train_windows.shape\n",
    "X_train_windows_flat = X_train_windows.reshape(n_samples, -1)\n",
    "\n",
    "print(f\"Aplicando SMOTE nas {n_samples} janelas de treino...\")\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_balanced_flat, y_train_balanced = sm.fit_resample(X_train_windows_flat, y_train_windows)\n",
    "\n",
    "# Remodelar os dados de volta para o formato 3D (amostras, timesteps, features)\n",
    "X_train_balanced = X_train_balanced_flat.reshape(-1, n_timesteps, n_features)\n",
    "\n",
    "print(f\"\\nFormato de X de treino após SMOTE: {X_train_balanced.shape}\")\n",
    "print(f\"Distribuição de classes após SMOTE: {Counter(y_train_balanced)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e4a8f3",
   "metadata": {},
   "source": [
    "## 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60  # 60 time steps (minutes)\n",
    "n_features = X.shape[1]  # number of features\n",
    "\n",
    "# model parameters\n",
    "input_shape = (window_size, n_features)\n",
    "n_classes = 1\n",
    "\n",
    "# 1D-CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    outputs = Dense(n_classes, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec5c5c",
   "metadata": {},
   "source": [
    "## InceptionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45785e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionTime model\n",
    "def create_inceptiontime_model(input_shape, n_filters=32, kernel_sizes=[3, 5, 11]):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Inception module\n",
    "    conv_list = []\n",
    "    for ks in kernel_sizes:\n",
    "        conv = Conv1D(filters=n_filters, kernel_size=ks, padding='same', activation='relu')(inputs)\n",
    "        conv_list.append(conv)\n",
    "    \n",
    "    x = Concatenate()(conv_list)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(n_classes, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
